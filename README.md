# Native Language Identification (NLI) of Indian English Speakers  
### Using HuBERT, MFCC, and Deep Learning

---

## ğŸ“Œ Table of Contents
- [Project Overview](#project-overview)
- [Dataset](#dataset)
- [Project Structure](#project-structure)
- [Feature Extraction](#feature-extraction)
  - [MFCC Extraction](#mfcc-extraction)
  - [HuBERT Embeddings](#hubert-embeddings)
- [Models](#models)
  - [MLP Classifier](#mlp-classifier)
  - [LSTM Model](#lstm-model)
  - [HuBERT Layer Analysis](#hubert-layer-analysis)
- [Training Pipeline](#training-pipeline)
- [Results](#results)
  - [MFCC vs HuBERT](#mfcc-vs-hubert)
  - [Age Generalization](#age-generalization)
  - [Word vs Sentence Analysis](#word-vs-sentence-analysis)
- [Visualization Outputs](#visualization-outputs)
- [Checkpoints](#checkpoints)
  - [How to Load Checkpoints](#how-to-load-checkpoints)
- [How to Run](#how-to-run)
  - [Google Colab Version](#google-colab-version)
- [Future Work](#future-work)
- [License](#license)

---

## ğŸ“Œ Project Overview
Native Language Identification (NLI) aims to classify the **native language (L1)** of Indian English speakers based on accent patterns.  
This project compares **traditional MFCC acoustic features** with **self-supervised HuBERT representations**.

We evaluate:
- Accent cues captured by MFCCs  
- HuBERT layer-wise representation quality  
- Age-based generalization  
- Word-level vs sentence-level speech  

---

## ğŸ“Œ Dataset
This project uses:

### **IndicAccentDB**
- Contains English audio by Indian speakers  
- Includes metadata (region, age, gender, etc.)  
- Balanced across 8+ native languages  

Loaded using HuggingFace:

```python

ğŸ“Œ Project Structure
ğŸ“ NLI_Project
â”‚â”€â”€ dataset/
â”‚â”€â”€ models/
â”‚â”€â”€ checkpoints/
â”‚â”€â”€ notebooks/
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ mfcc_extraction.py
â”‚   â”œâ”€â”€ hubert_features.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ evaluate.py
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt

ğŸ“Œ Feature Extraction
### ğŸ§ MFCC Extraction

40-dimensional MFCCs

Frame size: 25 ms

Hop length: 10 ms

import librosa
mfcc = librosa.feature.mfcc(y, sr=16000, n_mfcc=40)

### ğŸ¤– HuBERT Embeddings

Using:

facebook/hubert-large-ll60k


Extracting hidden states from all 24 layers.

processor = Wav2Vec2Processor.from_pretrained("facebook/hubert-large-ll60k")
model = HubertModel.from_pretrained("facebook/hubert-large-ll60k")

ğŸ“Œ Models
### ğŸ”¹ MLP Classifier

Used mainly for MFCC feature classification.

### ğŸ”¹ LSTM Model

Processes temporal sequences of MFCC/HuBERT features.

### ğŸ”¹ HuBERT Layer Analysis

We analyze which HuBERT hidden layer gives best performance.

Example:

Layer 9 â†’ Best for phonetic info

Layer 19 â†’ Best for accent classification

ğŸ“Œ Training Pipeline

Load dataset

Extract MFCC / HuBERT features

Train models

Evaluate

Generate plots

Save checkpoints

ğŸ“Œ Results
### â­ MFCC vs HuBERT
Feature	Accuracy
MFCC	~70%
HuBERT Layer 19	~89%
HuBERT Mean-pooled	~82%
### ğŸ‘¶ Age Generalization

Models trained on adults generalize well to 10â€“17 age group with HuBERT features.

### ğŸ—£ Word vs Sentence Analysis

Sentence-level recordings give higher accuracy.

ğŸ“Œ Visualization Outputs

Generated automatically:

Confusion Matrix

Training Curves

Layer-wise HuBERT Accuracy Plot

Age-group Comparison

MFCC vs HuBERT Comparison

ğŸ“Œ Checkpoints

Saved in:

/checkpoints/


Includes:

mfcc_mlp.pt

hubert_lstm.pt

hubert_layer_19.pt

hubert_mean_pool.pt

### ğŸ”§ How to Load Checkpoints
model.load_state_dict(torch.load("checkpoints/hubert_layer_19.pt"))
model.eval()

ğŸ“Œ How to Run

Install dependencies:

pip install -r requirements.txt


Run the main script:

python src/train.py

ğŸ“Œ Google Colab Version

â¡ï¸ Upload the notebook:
IndicAccent_NLI_HuBERT_MFCC.ipynb

Run all cells â€” code is fully compatible with Colab.

ğŸ“Œ Future Work

Add wav2vec2 and WavLM comparison

Add speaker diarization

Explore multilingual NLI

Improve dataset balancing

ğŸ“Œ License

MIT License
Free for research & educational use.


---

# âœ… YOUR README IS READY

If you want, I can also:

âœ” convert it into a **PDF**  
âœ” add **badges** (Python, HuggingFace, PyTorch, Colab)  
âœ” add **images or diagrams**  
âœ” add **your checkpoints section with links**

Just tell me **â€œgenerate PDFâ€** or **â€œadd badgesâ€** etc.
from datasets import load_dataset
dataset = load_dataset("DarshanaS/IndicAccentDb")
